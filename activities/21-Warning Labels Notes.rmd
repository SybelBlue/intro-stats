# Warning labels


\



**Goals**

- Practice hypothesis testing.    
- Explore common abuses of hypothesis testing so that (1) you don't make these mistakes yourself; and (2) you can read others' work with a healthy critical eye.    
- Review. The exercises focus on new concepts, yet they require the understanding of "old" material. Though you won't be required to do so, make sure you could write down model formulas, make predictions, interpret coefficients, etc if asked.    

\
\


**Directions**    

- These exercises will be due as Homework. The usual directions apply (eg: you must use the homework template).
- Make sure that all interpretations, explanations, etc are contextually meaningful / newspaper appropriate.    


\
\
\
\


## Exercises



```{r warning = FALSE, message = FALSE}
# Load packages
library(ggplot2)
library(dplyr)
library(magrittr)

# Define a function you'll need later
birthtest <- function(n){
  foods <- read.csv("https://www.macalester.edu/~ajohns24/data/food_list.csv")
  nleft <- rbinom(132, size = n, prob = 0.1)
  nright = n - nleft
  data <- data.frame(food = foods[,1], left = nleft, right = nright)
  pvals = rep(0,132)
  nleft = data$left 
  for(i in 1:132){
    pvals[i] = prop.test(x = nleft[i], n, p = 0.1)$p.value
  }
  data %>% 
    mutate(p_value = pvals)
}
```


\
\




## Exercise 1: Warm-up: gray area

Scroll down and check out the list in [this article](https://mchankins.wordpress.com/2013/04/21/still-not-significant-2/). Why is this funny? 

The reek of desperation.

But actually people really wanting their work to be perceived as meaningful regardless of its actual meaning.

\
\


## Exercise 2: Don't forget about multicollinearity

Consider the following relationship of daily bike share ridership among "casual" riders (non-members) with the "feels like" temperature and actual temperature:    

```{r}
bikes <- read.csv("https://www.macalester.edu/~dshuman1/data/155/bike_share.csv")
bike_model <- lm(riders_casual ~ temp_feel + temp_actual, bikes)
summary(bike_model)
```

a. Review: plot the relationship among these three variables.
```{r}
bikes %>%
  ggplot(aes(x = temp_feel, y = riders_casual, color=temp_actual-temp_feel)) +
  geom_point()
```

b. Which of the following null hypotheses corresponds to the p-value reported in the `temp_feel` row of the model summary table?       
    - $H_0$: there's an association between ridership and "feels like" temperature    
    - $H_0$: there's no association between ridership and "feels like" temperature    
    - $H_0$: when controlling for actual temperature, there's an association between ridership and "feels like" temperature     
    - *$H_0$: when controlling for actual temperature, there's no association between ridership and "feels like" temperature*

c. Your friend makes an easy mistake. Noting its large p-value, they conclude that ridership isn't associated with "feels like" temperature. Construct a new model that illustrates your friend's mistake, report the model summary table, and provide specific evidence from this table that contradicts your friend's mistake.    

```{r}
bike_model_2 <- lm(riders_casual ~ temp_feel, bikes)
summary(bike_model_2)

bikes %>%
  ggplot(aes(x = temp_feel, y = riders_casual)) +
  geom_point()
```

d. Explain *why* this happened, ie. why the original model and your new model provide two different insights into the relationship between ridership and "feels like" temperature. Support your argument with a visualization.

Because `temp_actual` and `temp_feel` are multicolinear, the significance of each is lowered when they are considered together. Using just one or the other can show that one is significant.

```{r}
bikes %>%
  ggplot(aes(x = temp_actual, y = temp_feel)) +
  geom_jitter() +
  geom_smooth()
```


e. OPTIONAL: What's being tested in the `F-statistic` row of the model summary table and how is this actually helpful here?
    
How strong these predictors are together at modeling the results.


\
\




## Exercise 3: Statistical vs practical significance: part I    

Two researchers make a claim: songs in the latin genre are longer than those in the pop / edm genre. Thus they're interested in the population model of song `duration` (in seconds) by `latin_genre`:
    
$$\text{duration } = \beta_0 + \beta_1 \text{ latin_genreTRUE}$$

where population coefficients $\beta_0$ and $\beta_1$ are unknown.

a. Which set of hypotheses align with the researchers' claim?    
    - $H_0$: $\beta_1 = 0$ vs $H_a$: $\beta_1 < 0$    
    - $H_0$: $\beta_1 = 0$ vs $H_a$: $\beta_1 \ne 0$    
    - *$H_0$: $\beta_1 = 0$ vs $H_a$: $\beta_1 > 0$*


b. To test these hypotheses, Researcher 1 collects data on 20 songs. Define a new variable, `duration`, which measures song duration in seconds (not milliseconds).    

```{r}
spotify_small <- read.csv("https://www.macalester.edu/~ajohns24/data/spotify_example_small.csv") %>% 
    select(track_artist, track_name, duration_ms, latin_genre) %>%
    mutate(duration = duration_ms / 1000)
```   

c. Using the 68-95-99.7 Rule *and* `confint()`, construct AND interpret an approximate 95% confidence interval for the true population coefficient $\beta_1$ in the model of `duration` by `latin_genre`.
```{r}
spotify_model <- lm(duration ~ latin_genre, spotify_small)
summary(spotify_model)$coef

spotify_small %>%
  ggplot(aes(x = latin_genre, y = duration/60)) +
  geom_boxplot()

confint(spotify_model)
```


d. Using this confidence interval alone, what conclusion do you make about the hypotheses?    
    - We have statistically significant evidence that latin genre songs tend to be longer than pop / edm songs.        
    - *We do not have statistically significant evidence that latin genre songs tend to be longer than pop / edm songs.*


\
\


## Exercise 4: Statistical vs practical significance: part II

Researcher 2 has the same research question, but gathers a different set of data:    
    
```{r}
spotify_big <- read.csv("https://www.macalester.edu/~ajohns24/data/spotify_example_big.csv") %>% 
  select(track_artist, track_name, duration_ms, latin_genre)
```

Use this data for the remainder of this exercise.

a. How many songs did Researcher 2 collect?
```{r}
count(spotify_big)
```


b. Construct and comment on a visualization of the relationship between `duration` (in seconds) and `latin_genre`. NOTE: As you did above, you'll first need to define `duration`.
```{r}
# spotify_big <- spotify_big %>% ...
spotify_big %<>% mutate(duration = duration_ms / 1000)

spotify_big %>%
  ggplot(aes(x = latin_genre, y = duration / 60)) +
  geom_boxplot()
```

There appears to be no difference between genres, but there are many more outliers than in the small dataset.

c. Model `duration` by `latin_genre` and interpret the `latin_genreTRUE` coefficient. (Think: In the context of song listening, is this a large or small effect size?)

```{r}
spotify_model_2 <- lm(duration ~ latin_genre, spotify_big)
summary(spotify_model_2)$coef
```

There is likely a very slight postive correlation between latin-ness and duration.

\
\



## Exercise 5: Statistical vs practical significance: part III

Let's conclude Researcher 2's analysis.

a. Obtain and report the p-value for the researcher's hypothesis test.
\[p = 0.03647731 / 2 = 0.01823865\]

b. How can we interpret this p-value "p"?    
    - There's only a p% chance that latin genre songs tend to be longer than pop / edm songs.    
    - *If there were truly no difference in the duration of latin vs pop / edm songs (in the broader population of songs), there's only a p% chance that we'd have gotten a sample in which the observed difference was so large.*
    - There's only a p% chance that latin genre songs tend to be the same length as pop / edm songs.    


c. If the researchers made a yes-or-no decision using a 0.05 significance level, what would they say?    
    - *We have statistically significant evidence that latin genre songs tend to be longer than pop / edm songs.*
    - We do not have statistically significant evidence that latin genre songs tend to be longer than pop / edm songs.    


d. HUH?!?  You've just witnessed the stark difference between **statistical significance** and **practical significance**. Explain *why* this happened. That is, when might we observe statistically significant results that aren't practically significant?    
    NOTE:
    - This result is consistent with our simulations of Type II error rates and power.
    - This result seems silly, but is quite common in practice. Hence the importance of investigating statistical and practical significance hand-in-hand.

There is a significant indication that latin songs tend to be 1.5s longer than their edm/pop counterparts. This is not practically interesting.
    
\
\

## Exercise 6: Doing lots of tests can produce misleading conclusions    

Suppose researchers asked pregnant people about their consumption of 132 different foods. For each food, they then tested for an association of that food with having child that grows up to be left-handed (a figure which is roughly 10% among the general population):
    
$H_0$: proportion of children that are left-handed = 0.1    
$H_a$: proportion of children that are left-handed $\ne$ 0.1


(Seem silly? This scenario is based on a real article ["You are what your mother eats"](https://royalsocietypublishing.org/doi/full/10.1098/rspb.2008.0105) in the reputable journal, *Proceedings of the Royal Society B - Biological Sciences*, which asserted that eating cereal during pregnancy is associated with having a male baby.)    

In reality, NONE of these 132 foods are truly linked to handedness -- $H_0$ is true in all 132 cases. With this in mind, let's *simulate* what results the researchers might find. Specifically, for each of the 132 foods, simulate the handedness of the children born to 100 pregnant subjects that eat those foods. Be sure to set your random number seed to whatever positive integer you like:

```{r}
set.seed(28572028)
handedness_tests <- birthtest(n = 100)
head(handedness_tests)
```


a. Your `handedness_tests` data contains 132 rows, each corresponding to a different food. Scan your `handedness_tests` data (but do not print this out in your html!). We'd expect roughly 10 of 100 children to be left-handed. Which food deviates the most from this expectation?
```{r}
handedness_tests %>%
  select(food, p_value) %>%
  arrange(p_value) %>%
  head
```


b. For each food, we'll use our sample data to test whether left-handedness significantly differs from 0.1. In doing so we might make a Type I error. What does this mean in our setting?    
    - *Conclude that a food is linked to handedness when it's not.*
    - Conclude that a food is not linked to handedness when it is.


c. The p-values associated with each food are reported in `handedness_tests`. Identify the foods that correspond to Type I errors, ie. have a p-value < 0.05 despite the fact that $H_0$ is true (ie. no association with handedness).
```{r}
handedness_tests %>%
  select(food, p_value) %>%
  filter(p_value < 0.05)
```


d. For each food you identified in part c, write a fake headline trumpeting your findings.    
    
\[\mathcal{Sweet~Potatoes~Cause~Lefties!}\]

\
\




## Exercise 7: Multiple testing

Let's explore the math behind this phenomenon. Assume we conduct each hypothesis test at the 0.05 significance level. Thus in any *one* test, there's a 0.05 probability of making a Type I error when $H_0$ is true. What if we conduct *two* tests? Assuming $H_0$ is true for both, there are 4 possible combinations of conclusions we might make:

Test 1              Test 2              Probability
------------------- ------------------- ---------------------
not significant     not significant     $0.95*0.95 = 0.9025$
not significant     significant         $0.95*0.05 = 0.0475$
significant         not significant     $0.95*0.05 = 0.0475$
significant         significant         $0.05*0.05 = 0.0025$
TOTAL                                   1


\


a. Based on the above table, if we conduct two tests and $H_0$ is actually true for both, what's the probability we make **at least one** Type I error?

\[1 - 0.95*0.95 = 0.0975\]

b. In general, suppose we conduct "g" tests and that $H_0$ is true for each one. The probability of getting **at least one** Type I error is $1 - 0.95^g$. (NOTE: You neither need to prove nor memorize this, but you can if you have extra time!) This is called the *overall* or *family-wise* Type I error rate. Use this formula to calculate the following:    
    - overall Type I error rate when g = 1    
    - overall Type I error rate when g = 10    
    - overall Type I error rate when g = 100
```{r}
for (i in c(1, 10, 100)) {
  print(1 - 0.95^i)
}
```


c. Relatedly, explain why this cartoon is funny: [https://imgs.xkcd.com/comics/significant.png](https://imgs.xkcd.com/comics/significant.png)
    
I think it's funny because we know that **green** jellybeans don't cause acne any more than the other colors, so by law of large numbers we end up with an odd sample eventually, and that catches headlines (that also mis-interpret the results)
    
\
\



## Exercise 8: p-hacking

fivethirtyeight put together a [nice simulation](https://projects.fivethirtyeight.com/p-hacking/) that highlights the dangers of fishing for significance and the importance of always questioning: how many things did people test? can I replicate these results? what data did people use? what was their motivation?  how did they measure their variables? might their results have changed if they used different measurements?
    
Use this simulation to "prove" 2 different claims. For both, be sure to indicate what response variable (eg: Employment rate) and data (eg: senators) you used: 

- the economy is significantly better when Democrats are in power (all politicians w/ employment, GDP, and inflation)   
- the economy is significantly worse when Democrats are in power (presidents and governors w/ employment, inflation and stock market)
    


\
\




## Exercise 9: OPTIONAL: Bonferroni

NOTE: If you are a Bio major, you should give this a peak.  
\[\mathcal{No~Thanks!}\]  
